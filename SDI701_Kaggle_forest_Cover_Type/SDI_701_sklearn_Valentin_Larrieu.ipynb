{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectFpr, f_regression, f_classif\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is the best model I could build. Please look at the file \"SD701_Valentin_Larrieu_Report.docx\" \n",
    "for more informations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 : ExtraTrees with Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before drop (528720, 71)\n",
      "shape after drop (528720, 63)\n"
     ]
    }
   ],
   "source": [
    "# We need pandas dataframe for sklearn\n",
    "df_train = pd.read_csv('C:\\\\Users\\\\Orion\\\\Documents\\\\MS-BGD\\\\BigDataMining\\\\train-set.csv')\n",
    "df_test = pd.read_csv('C:\\\\Users\\\\Orion\\\\Documents\\\\MS-BGD\\\\BigDataMining\\\\test-set.csv')\n",
    "\n",
    "\n",
    "Y = df_train.Cover_Type\n",
    "\n",
    "# We drop the ID column because it do not give usefull information\n",
    "X = df_train.drop(['Id','Cover_Type'],axis=1)\n",
    "X_test_input = df_test.drop('Id',axis=1)\n",
    "\n",
    "#Feature creation\n",
    "X[\"distance\"] = np.sqrt(X.Horizontal_Distance_To_Hydrology**2 + X.Vertical_Distance_To_Hydrology**2)\n",
    "X[\"High\"] = X.Elevation+ X.Vertical_Distance_To_Hydrology \n",
    "X[\"Shade_mean\"] = (X.Hillshade_9am+X.Hillshade_Noon+X.Hillshade_3pm)/3\n",
    "X[\"slope_shade\"] = X.Slope/ X.Shade_mean\n",
    "X[\"elevation_shade\"] = X.Elevation/ X.Shade_mean\n",
    "X[\"slope_elevation\"] = X.Slope/ X.Elevation\n",
    "X['Hydro_slope'] = X.Vertical_Distance_To_Hydrology / X.Horizontal_Distance_To_Hydrology\n",
    "\n",
    "# We try to create features that can be separated verticaly\n",
    "X['Hydro_elev']=X.Elevation - 0.2 * X.Horizontal_Distance_To_Hydrology\n",
    "X['Road_elev']=X.Elevation - 0.05 * X.Horizontal_Distance_To_Roadways\n",
    "X['Hydro_elev_vert']=X.Elevation - X.Vertical_Distance_To_Hydrology\n",
    "X['Horiz_mean']=(X.Horizontal_Distance_To_Fire_Points + X.Horizontal_Distance_To_Hydrology + X.Horizontal_Distance_To_Roadways) / 3\n",
    "X['Horiz_fire_hydr']=(X.Horizontal_Distance_To_Fire_Points + X.Horizontal_Distance_To_Hydrology) / 2\n",
    "X['Horiz_hydr_road']=(X.Horizontal_Distance_To_Roadways + X.Horizontal_Distance_To_Hydrology) / 2\n",
    "X['Horiz_road_fire']=(X.Horizontal_Distance_To_Roadways + X.Horizontal_Distance_To_Hydrology) / 2\n",
    "X['Horiz_diff_fire_hydr']=abs(X.Horizontal_Distance_To_Fire_Points - X.Horizontal_Distance_To_Hydrology)\n",
    "X['Horiz_diff_hydr_road']=abs(X.Horizontal_Distance_To_Roadways - X.Horizontal_Distance_To_Hydrology)\n",
    "X['Horiz_diff_road_fire']=abs(X.Horizontal_Distance_To_Roadways - X.Horizontal_Distance_To_Hydrology)\n",
    "\n",
    "\n",
    "X_test_input[\"distance\"] = np.sqrt(X_test_input.Horizontal_Distance_To_Hydrology**2 + X_test_input.Vertical_Distance_To_Hydrology**2)\n",
    "X_test_input[\"High\"] = X_test_input.Elevation+ X_test_input.Vertical_Distance_To_Hydrology \n",
    "X_test_input[\"Shade_mean\"] = (X_test_input.Hillshade_9am+X_test_input.Hillshade_Noon+X_test_input.Hillshade_3pm)/3\n",
    "X_test_input[\"slope_shade\"] = X_test_input.Slope/ X_test_input.Shade_mean\n",
    "X_test_input[\"elevation_shade\"] = X_test_input.Elevation/ X_test_input.Shade_mean\n",
    "X_test_input[\"slope_elevation\"] = X_test_input.Slope/ X_test_input.Elevation \n",
    "X_test_input['Hydro_slope'] = X_test_input.Vertical_Distance_To_Hydrology / X_test_input.Horizontal_Distance_To_Hydrology\n",
    "X_test_input['Hydro_elev']=X_test_input.Elevation - 0.2 * X_test_input.Horizontal_Distance_To_Hydrology\n",
    "X_test_input['Road_elev']=X_test_input.Elevation - 0.05 * X_test_input.Horizontal_Distance_To_Roadways\n",
    "X_test_input['Hydro_elev_vert']=X_test_input.Elevation - X_test_input.Vertical_Distance_To_Hydrology\n",
    "X_test_input['Horiz_mean']=(X_test_input.Horizontal_Distance_To_Fire_Points + X_test_input.Horizontal_Distance_To_Hydrology + X_test_input.Horizontal_Distance_To_Roadways) / 3\n",
    "X_test_input['Horiz_fire_hydr']=(X_test_input.Horizontal_Distance_To_Fire_Points + X_test_input.Horizontal_Distance_To_Hydrology) / 2\n",
    "X_test_input['Horiz_hydr_road']=(X_test_input.Horizontal_Distance_To_Roadways + X_test_input.Horizontal_Distance_To_Hydrology) / 2\n",
    "X_test_input['Horiz_hydr_road']=(X_test_input.Horizontal_Distance_To_Roadways + X_test_input.Horizontal_Distance_To_Hydrology) / 2\n",
    "X_test_input['Horiz_road_fire']=(X_test_input.Horizontal_Distance_To_Roadways + X_test_input.Horizontal_Distance_To_Hydrology) / 2\n",
    "X_test_input['Horiz_diff_fire_hydr']=abs(X_test_input.Horizontal_Distance_To_Fire_Points - X_test_input.Horizontal_Distance_To_Hydrology)\n",
    "X_test_input['Horiz_diff_hydr_road']=abs(X_test_input.Horizontal_Distance_To_Roadways - X_test_input.Horizontal_Distance_To_Hydrology)\n",
    "X_test_input['Horiz_diff_road_fire']=abs(X_test_input.Horizontal_Distance_To_Roadways - X_test_input.Horizontal_Distance_To_Hydrology)\n",
    "\n",
    "# We remove the infinite values the column division could have created\n",
    "X.Shade_mean=X.Shade_mean.map(lambda x: 0 if np.isinf(x) else x)\n",
    "X.slope_shade=X.slope_shade.map(lambda x: 0 if np.isinf(x) else x)\n",
    "X.elevation_shade=X.elevation_shade.map(lambda x: 0 if np.isinf(x) else x)\n",
    "X.Hydro_slope=X.Hydro_slope.map(lambda x: 0 if np.isinf(x) else x)\n",
    "X.slope_elevation=X.slope_elevation.map(lambda x: 0 if np.isinf(x) else x)\n",
    "\n",
    "X_test_input.Shade_mean=X_test_input.Shade_mean.map(lambda x: 0 if np.isinf(x) else x)\n",
    "X_test_input.slope_shade=X_test_input.slope_shade.map(lambda x: 0 if np.isinf(x) else x)\n",
    "X_test_input.elevation_shade=X_test_input.elevation_shade.map(lambda x: 0 if np.isinf(x) else x)\n",
    "X_test_input.Hydro_slope=X_test_input.Hydro_slope.map(lambda x: 0 if np.isinf(x) else x)\n",
    "X_test_input.slope_elevation=X_test_input.slope_elevation.map(lambda x: 0 if np.isinf(x) else x)\n",
    "\n",
    "X[X==np.inf] = np.nan\n",
    "X.fillna(X.mean(), inplace=True)\n",
    "X_test_input[X_test_input==np.inf] = np.nan\n",
    "X_test_input.fillna(X_test_input.mean(), inplace=True)\n",
    "\n",
    "#Feature selection\n",
    "print(\"shape before drop\", X.shape)\n",
    "\n",
    "# Normalisation of the data\n",
    "sc = StandardScaler()\n",
    "model_centered = sc.fit(X)\n",
    "X = model_centered.transform(X)\n",
    "model_centered = sc.fit(X_test_input)\n",
    "X_test_input = model_centered.transform(X_test_input)\n",
    "\n",
    "# Feature selection\n",
    "sel = SelectFpr(f_regression,alpha=0.000001)\n",
    "model_sel = sel.fit(X,Y)\n",
    "X = sel.transform(X)\n",
    "X_test_input = sel.transform(X_test_input)\n",
    "\n",
    "print(\"shape after drop\", X.shape)\n",
    "\n",
    "# We split our data\n",
    "X_train,X_test,Y_train,Y_test = model_selection.train_test_split(X,Y,test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.96      0.95      0.96     19278\n",
      "          2       0.96      0.97      0.96     25709\n",
      "          3       0.95      0.96      0.96      3325\n",
      "          4       0.90      0.89      0.89       230\n",
      "          5       0.91      0.82      0.86       899\n",
      "          6       0.94      0.91      0.92      1542\n",
      "          7       0.97      0.95      0.96      1889\n",
      "\n",
      "avg / total       0.96      0.96      0.96     52872\n",
      "\n",
      "ExtraTrees Accuracy : 0.9578037524587684\n"
     ]
    }
   ],
   "source": [
    "# We fit our model\n",
    "et = AdaBoostClassifier(ExtraTreesClassifier(n_estimators=300, criterion= 'entropy', n_jobs = -1, warm_start=True, max_features = 19), n_estimators=300, learning_rate=0.001, algorithm='SAMME.R')\n",
    "\n",
    "et.fit(X_train,Y_train)\n",
    "\n",
    "# We use it to predict our output\n",
    "Y_hat = et.predict(X_test)\n",
    "\n",
    "# We print the results\n",
    "print(metrics.classification_report(Y_test,Y_hat))\n",
    "print(\"ExtraTrees Accuracy :\", metrics.accuracy_score(Y_test,Y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We retrain our model with our entire set to have the best model for kaggle\n",
    "et2 = AdaBoostClassifier(ExtraTreesClassifier(n_estimators=300, criterion= 'entropy', n_jobs = -1, warm_start=True, max_features = 19), n_estimators=300, learning_rate=0.01, algorithm='SAMME.R')\n",
    "\n",
    "et2.fit(X,Y)\n",
    "Y_hat_export2 = et2.predict(X_test_input)\n",
    "\n",
    "export_df2 = pd.DataFrame({'Id':df_test.Id.values,'Cover_Type':Y_hat_export2}).sort_index(ascending=False,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_df2.to_csv('C:\\\\Users\\\\Orion\\\\Documents\\\\MS-BGD\\\\BigDataMining\\\\submission_local.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
